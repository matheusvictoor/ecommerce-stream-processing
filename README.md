# üõí E-commerce Real-Time Stream Processing

**Disciplina**: Arquitetura de Software \
**Curso**: Ci√™ncia da Computa√ß√£o \
**Institui√ß√£o**: Universidade Federal de Campina Grande - UFCG

## üìå Vis√£o Geral
Este projeto implementa uma pipeline completa de processamento de stream em tempo real para an√°lise de transa√ß√µes de e-commerce. A arquitetura segue os princ√≠pios de desacoplamento, escalabilidade, observabilidade e resili√™ncia, utilizando tecnologias modernas de streaming e armazenamento.

- A pipeline simula transa√ß√µes de compras, as processa em janelas de 1 minuto, agrega m√©tricas de vendas e persiste os resultados em dois sistemas distintos:

- PostgreSQL para armazenamento transacional e consultas SQL
Elasticsearch + Kibana para visualiza√ß√£o interativa e an√°lise em tempo real

## üèóÔ∏è Arquitetura da Solu√ß√£o
![alt text](architecture.png)

### Componentes

|  COMPONENTE         |                         FUNC√ÇO                                 |
|---------------------|----------------------------------------------------------------|
| **Locust**          | Gera carga simulando usu√°rios reais (transa√ß√µes de compra)     |
| **Producer**        | API REST (Flask) que recebe transa√ß√µes e envia para o Kafka    |
| **Kafka**           | Sistema de mensageria distribu√≠do (t√≥pico: transactions)       |
| **Zookeeper**       | Coordena√ß√£o do cluster Kafka (obrigat√≥rio na vers√£o 7.4 usada) |
| **Flink**           | Motor de processamento de stream (JobManager + TaskManager)    |
| **PostgreSQL**      | Banco relacional para armazenamento estruturado                |
| **Elasticsearch**   | Motor de busca/an√°lise para dados n√£o estruturado              |
| **Kibana**          | Interface de visualiza√ß√£o e dashboard                          |
| **Kafka UI**        | Interface web para monitoramento de t√≥picos e mensagens        |


## üì¶ Estrutura do Projeto
BASE-STREAM-PROCESSING/

Em breve


## üöÄ Tecnologias

- Apache Flink
- Kafka
- Locust
- PostgreSQL
- Elasticsearch + Kibana
- Docker + Docker Compose
- GitHub Actions (CI/CD)

## ‚ñ∂Ô∏è Como Executar
Pr√©-requisitos
- Docker e Docker Compose instalados
- Acesso √† internet (para baixar imagens e JARs)

Passo a passo

1. Baixar os conectores necess√°rios (fa√ßa isso uma vez):
```bash
# Na pasta raiz do projeto
mkdir -p flink-job/lib

# Conector JDBC
wget https://repo1.maven.org/maven2/org/apache/flink/flink-connector-jdbc/3.1.0-1.17/flink-connector-jdbc-3.1.0-1.17.jar -P flink-job/lib/

# Conector Elasticsearch 7
wget https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-elasticsearch7_2.12/1.18.1/flink-sql-connector-elasticsearch7_2.12-1.18.1.jar -P flink-job/lib/

# Driver JDBC do PostgreSQL
wget https://jdbc.postgresql.org/download/postgresql-42.7.3.jar -P flink-job/lib/
```

2. Iniciar toda a stack:
```bash
chmod +x scripts/*.sh
./scripts/start-all.sh
```

3. Acessar os servi√ßos:

|    SERVI√áO          |                    URL                   |
|---------------------|------------------------------------------|
| **Locust**          |     http://localhost:8089                |
| **Kafka UI**        |     http://localhost:8080                |
| **Flink Web UI**    |     http://localhost:8081                |
| **Kibana**          |     http://localhost:5601                |

4. (Opcional) Executar teste de carga manual:
```bash
./scripts/load_test.sh
```

## ‚úÖ Valida√ß√£o da Pipeline

Ele verifica:

- Se h√° dados na tabela sales_summary do PostgreSQL
- Se h√° documentos no √≠ndice transactions-aggregated do Elasticsearch
- Se o Kibana est√° respondendo

Resultado esperado: üéâ **PIPELINE VALIDADA COM SUCESSO!**


## üìä Visualiza√ß√£o no Kibana
1. Acesse http://localhost:5601

2. V√° em Stack Management ‚Üí Data Views ‚Üí Create data view
    - Name: transactions-*
    - Index pattern: transactions-*
    - Timestamp field: @timestamp
3. V√° em Analytics ‚Üí Discover e explore os dados

4. Crie dashboards com:
    - Receita total por categoria
    - Vendas por m√©todo de pagamento
    - Evolu√ß√£o temporal de vendas

## üß™ Testes Automatizados

Na raiz do projeto, rode o seguinte comando para rodar os testes:
```bash
./tests/load_test.sh
python tests/validate_pipeline.py
```

## üõ†Ô∏è Tecnologias Utilizadas

|      CATEGORIA      |                         TECNOLOGIA                                 |   VERS√ÇO
|---------------------|----------------------------------------------------------------|
| **Locust**          | Gera carga simulando usu√°rios reais (transa√ß√µes de compra)     |
Mensageria
Apache Kafka + Zookeeper
7.4.0
Processamento
Apache Flink (PyFlink)
1.18.1
Armazenamento
PostgreSQL
17
Busca/Visualiza√ß√£o
Elasticsearch + Kibana
8.11.0
Gera√ß√£o de Carga
Locust
2.25.0
API
Flask
2.3.3
Orquestra√ß√£o
Docker Compose
‚Äî

## üë• Contribuidores
- Matheus Victor Pereira
- Aline de Brito das Neves
- Johansson Lucena
- Igor Ribeiro de Souza
- Erick Araken Vieira Gomes


## üìö Refer√™ncias
- Apache Flink Documentation: https://nightlies.apache.org/flink/flink-docs-release-1.18/
- Elasticsearch + Flink Connector: https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/elasticsearch/
- Locust: https://locust.io/
- Kafka UI: https://github.com/kafbat/kafka-ui